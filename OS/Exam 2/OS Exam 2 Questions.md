# OS Exam 2 Questions

## Deadlocks (Chapter 7)

### Necessary conditions

1. Mutex
    - At least one resource must be held in a non-sharable mode
    - Only one process at a time can use the resource
2. Hold and wait
    - A proc must be holding 1+ resources and waiting on more that are held up by other procs
3. No preemption
    - Resources can't be preempted
      - Must be voluntarily released by process holding it
4. Circular wait
    - Cycle is preset (P0->R0->P1->R1->P0)
    - Happens when only one instance of resource type

### Resource-Allocation Graph

- Processes (circles) and resources (squares) are vertices
- Edges show relationship
  - Request: P_i -> R_i
  - Assignment: R_i -> P_i
- Resources with multiple instances have dots representing the number of instances inside their squares
- If cycle, possible deadlock
  - If cycle and a resource type has a single instance and is requested by multiple, then deadlock occurs

### Prevention

- Just need to prevent one of the four necessary conditions from happening
- Mutex: Can't help
- Hold and Wait: guarantee that when proc requests a resource, it doesn't hold other resources
- No preemption: if proc requests resources but must wait for them, preempt its currently held resources
  - The held resources are added to list of resources being waited on
- Circular wait: enumerate resources, use resources in increasing order

### Avoidance

- Disadvantages of preventative methods include low device utilization and system throughput
- Use information ahead of time to determine if proc should wait to avoid deadlock
  - Information: max number of each resource type needed
    - Leads into banker's algorithm

#### Banker's

- Known:
  - Available resources (vector of length _m_)
  - Allocation: _n_ x _m_ matrix, defines # of resources of each type allocated to each proc to begin with
  - Max (requested): _n_ x _m_ matrix, defines total resource need for each resource type and each proc
- Unknown:
  - Need: _n_ x _m_ matrix, (Max - allocation)

### Detection

![Detection algorithm](https://i.imgur.com/y4AkURY.png)

### Wait-for graph

### Recovery

- Two methods:
  - Abort **all**
    - Big expense
  - Abort **one at a time** until deadlock cycle gone
    - Big overhead
    - Choose one victim, suspend its resources, rollback (restart) check again for deadlock
      - Victim factors:
        1. Priority
        2. Total computation time thus far
        3. How many/what kind of resources (simple to preempt?)
        4. How many resources left until complete
        5. How many procs will need termination
        6. Proc interactive or batch
      - If deadlock still, choose second victim
    - Disadvantage: starvation of processes
      - One process may be preempted every time
        - Solve by factoring in # of rollbacks

## Main memory (Chapter 8)

### Address binding

1. Compile time
    - If you know where the process will reside in memory
    - Absolute location code
    - Train ticket example (know exactly what cabin you have)
2. Load Time
    - If you don't know where the process will go
    - Relocatable code
    - Plane ticket example (don't know exact seat until boarding pass)
3. Execution time
    - If process is moved during execution
    - Bus ticket example (can move around en route)

### Logical/physical space

- Logical: address generated by CPU
  - AKA virtual address
  - Only type seen by user program
- Physical: address seen by memory unit
- Compile & load time binding generate the same logical and physical addresses
  - Execution time binding gives different addresses
    - Differing address __spaces__
- Mapping from virtual->physical taken care of by memory management unit (MMU)
- Use __dynamic loading__ for better memory space utilization
  - Like lazy loading, subroutines only loaded from disk when called
  - Good for large programs and when memory is small
  - __Dynamic linking__ is similar, but is done at execution time
    - Prevent libraries from having to be included in every program's binary that needs it
      - Dynamically linked libraries (DLLs)
    - __Stub__ (short library loading code block) used for each reference
    - Works well for version updates; any references will be automatically updated

### Allocation

- Protection
  - Don't want process to access memory it doesn't own
  - Include a limit register that its logical address must abide by
- Partitions
  - Fixed-size
    - One process for each
  - Variable size
    - Starts with one big hole of memory
      - Filled up by processes, left with various-sized holes
      - OS picks hole for process in input queue
        - Hole chosen with scheduling algorithm (best fit, worst fit, first fit)
          - __Dynamic storage allocation problem__
        - If hole too large, split in two
          - Merge contiguous holes

#### Fragmentation

- **External**
  - As processes come and go, free memory space is broken into small pieces
  - External fragmentation happens when enough total memory for request, but spaces not contiguous
  - Happens with first-fit and best-fit strategies
  - __50% rule__: with _N_ allocated blocks, another 0.5 _N_ blocks will be lost to fragmentation
  - Solution: __compaction__
    - Rearrange all free memory into one large block
    - Possible only if relocation is dynamic and done at execution time
      - Can just relocate and change base register
    - Not possible if relocation is static and done at assembly or load time
    - Can be expensive
  - Other solution: allow non-contiguous memory for process
    - Achieved through __segmentation__ or __paging__
- **Internal**
  - Fragmentation that's internal to a partition
  - Happens with fixed partitions when process doesn't use whole partition size

### Segmentation

- Maps programmer's view to actual physical memory
- A logical address space is a collection of segments
  - Segment has name, length, number, and offset
    - logical address: <segment-number, offset>
- Map 2D logical address to 1D physical address using __segment table__
  - Table is array of base-limit register pairs
  - Offset must not go beyond segment's limit

### Paging

- Like segmentation, this allows process' physical address space to be non-contiguous
  - Unlike segmentatin, paging avoids external fragmentation and need for compaction
    - Also solves problem of fitting various-sized memory chunks in backing store during swapping
      - Backing store has fragmentation problems and access is slow so compaction is impossible

#### Method of implementation

- Split physical memory into fixed-sized blocks called **frames**
- Split logical memory into same-sized blocks called __pages__
- Load process' pages into any available frames when ready to execute
- Backing store spit into fixed-sized blocks same size as frames for clusters of frames
- Advantage: logical space can be bigger than physical space
- Logical address: __page number (p)__ and __page offset (d)__
  - Page number used as index in __page table__
    - Page table contains base address of each page in physical memory
      - Combine with offset to define real address
- ![Page table](https://i.imgur.com/6yuid2f.png)

#### EAT

- If miss: two mem access + access time to TLB

#### Memory protection in paging system

#### Diff. strucutres of page table

- Hierachical (multilevel)
- Inverted
- Disadvantages

#### Segmentation

- Protection

#### Fragmentation

#### Advantages of paging and segmentation

- Sharing/protection easier in segmentation system

## Virtual Memory

### Benefits


### Demand paging


### Steps to handle page fault


### EAT


### Copy-on-write

- Child process involved

### Page replacement methods


### Second change algo


### Allocations of frames


### Alloc algos


### Global vs. local replacement

- Global: choose victim from any process
- Local: choose victim that's making new reference in a specific process (helps with thrashing)

### Thrashing


### Working set model


### Prepaging


### Page size implications (on fragmentation, page table size, etc.)

### Program structure

